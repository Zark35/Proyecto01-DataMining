{"block_file": {"custom/test2.py:custom:python:test2": {"content": "return {\n    \"pg_host\": get_secret_value(\"pg_host\"),\n    \"pg_user\": get_secret_value(\"pg_user\"),\n    \"qb_realm_id\": get_secret_value(\"qb_realm_id\")\n}\n", "file_path": "custom/test2.py", "language": "python", "type": "custom", "uuid": "test2"}, "data_exporters/data_esporter_invoices.py:data_exporter:python:data esporter invoices": {"content": "from mage_ai.data_preparation.decorators import data_exporter\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, Table, Column, String, Integer, DateTime, JSON, MetaData\nfrom sqlalchemy.dialects.postgresql import insert\n\n\ndef get_table(table_name: str):\n    \"\"\"\n    Define la tabla qb_invoices con esquema est\u00e1ndar raw\n    \"\"\"\n    metadata = MetaData()\n    return Table(\n        table_name, metadata,\n        Column('id', String, primary_key=True),\n        Column('payload', JSON),\n        Column('ingested_at_utc', DateTime),\n        Column('extract_window_start_utc', String),\n        Column('extract_window_end_utc', String),\n        Column('page_number', Integer),\n        Column('page_size', Integer),\n        Column('request_payload', String),\n        extend_existing=True\n    )\n\n\n@data_exporter\ndef export_to_postgres(data, *args, **kwargs):\n    \"\"\"\n    Data Exporter:\n    - Inserta los registros en Postgres\n    - UPSERT por id (idempotencia)\n    \"\"\"\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    table = get_table('qb_invoices')\n    with engine.begin() as conn:\n        for rec in data:\n            stmt = insert(table).values(**rec)\n            stmt = stmt.on_conflict_do_update(\n                index_elements=['id'],\n                set_={k: stmt.excluded[k] for k in rec.keys() if k != 'id'}\n            )\n            conn.execute(stmt)\n\n    print(f\"\u2705 {len(data)} registros exportados con UPSERT en qb_invoices\")\n", "file_path": "data_exporters/data_esporter_invoices.py", "language": "python", "type": "data_exporter", "uuid": "data_esporter_invoices"}, "data_loaders/data_loader_invoices.py:data_loader:python:data loader invoices": {"content": "import requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom mage_ai.data_preparation.decorators import data_loader\nfrom datetime import datetime, timedelta\n\n\ndef refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n    auth = (client_id, client_secret)\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    resp = requests.post(url, headers=headers, data=data, auth=auth, timeout=30)\n    resp.raise_for_status()\n    return resp.json()['access_token']\n\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version):\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n    params = {\"query\": query, \"minorversion\": minor_version}\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n    resp = requests.get(url, headers=headers, params=params, timeout=60)\n    resp.raise_for_status()\n    return resp.json()\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Data Loader para QuickBooks Invoices\n    \"\"\"\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-07')\n\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    minor_version = 75\n\n    start = datetime.fromisoformat(fecha_inicio)\n    end = datetime.fromisoformat(fecha_fin)\n\n    all_records = []\n\n    chunk_start = start\n    while chunk_start <= end:\n        chunk_end = min(chunk_start + timedelta(days=1), end)\n        query = f\"select * from Invoice where TxnDate >= '{chunk_start.date()}' and TxnDate <= '{chunk_end.date()}'\"\n\n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        invoices = data.get(\"QueryResponse\", {}).get(\"Invoice\", [])\n\n        print(f\"\ud83d\udcc5 Rango {chunk_start.date()} \u2192 {chunk_end.date()} | {len(invoices)} registros extra\u00eddos\")\n\n        for rec in invoices:\n            all_records.append({\n                \"id\": rec.get(\"Id\"),\n                \"payload\": rec,\n                \"ingested_at_utc\": datetime.utcnow().isoformat(),\n                \"extract_window_start_utc\": chunk_start.isoformat(),\n                \"extract_window_end_utc\": chunk_end.isoformat(),\n                \"page_number\": 1,\n                \"page_size\": len(invoices),\n                \"request_payload\": query\n            })\n\n        chunk_start = chunk_end + timedelta(days=1)\n\n    return all_records\n", "file_path": "data_loaders/data_loader_invoices.py", "language": "python", "type": "data_loader", "uuid": "data_loader_invoices"}, "data_loaders/qb_customers_backfil.py:data_loader:python:qb customers backfil": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@data_loader\ndef load_customers(*args, **kwargs):\n    \"\"\"\n    Carga clientes desde QuickBooks API usando OAuth 2.0.\n    \"\"\"\n    # 1. Obtener credenciales desde Secrets\n    client_id = get_secret_value(\"qb_client_id\")\n    client_secret = get_secret_value(\"qb_client_secret\")\n    refresh_token = get_secret_value(\"qb_refresh_token\")\n    realm_id = get_secret_value(\"qb_realm_id\")\n\n    # 2. Obtener un access token nuevo desde el refresh token\n    auth = (client_id, client_secret)\n    resp = requests.post(\n        \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\",\n        auth=auth,\n        headers={\"Accept\": \"application/json\", \"Content-Type\": \"application/x-www-form-urlencoded\"},\n        data={\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n    )\n    access_token = resp.json()[\"access_token\"]\n\n    # 3. Llamar a la API de QuickBooks para obtener Customers\n    url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query\"\n    query = \"select * from Customer\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/text\"\n    }\n    response = requests.post(url, headers=headers, data=query)\n\n    data = response.json()\n    return data\n", "file_path": "data_loaders/qb_customers_backfil.py", "language": "python", "type": "data_loader", "uuid": "qb_customers_backfil"}, "data_loaders/test1.py:data_loader:python:test1": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    return {\n        \"pg_host\": get_secret_value(\"pg_host\"),\n        \"pg_user\": get_secret_value(\"pg_user\"),\n        \"qb_realm_id\": get_secret_value(\"qb_realm_id\"),\n    }\n", "file_path": "data_loaders/test1.py", "language": "python", "type": "data_loader", "uuid": "test1"}, "data_loaders/test2.py:data_loader:python:test2": {"content": "import requests\nimport pandas as pd\nimport json\nfrom sqlalchemy import create_engine\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef refresh_access_token():\n    \"\"\"\n    Refresca el access_token de QuickBooks usando el refresh_token\n    \"\"\"\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n\n    auth = (client_id, client_secret)  # Basic Auth con client_id y secret\n\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data, auth=auth)\n        response.raise_for_status()\n        tokens = response.json()\n\n        # El nuevo access token\n        new_access_token = tokens['access_token']\n\n        print(\"\u2705 Access token refrescado correctamente\")\n        return new_access_token\n\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Error al refrescar el access token: {e}\")\n        return None\n\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version):\n    \"\"\"\n    Llama a la API de QuickBooks y devuelve el JSON con los resultados\n    \"\"\"\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor version\")\n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    params = {\n        'query': query,\n        'minorversion': minor_version\n    }\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n    try:\n        response = requests.get(url, headers=headers, params=params, timeout=60)\n        response.raise_for_status()\n        data = response.json()\n        print('\u2705 Datos recibidos correctamente de QuickBooks')\n        return data\n    except requests.exceptions.RequestException as e:\n        print(f'\u274c Error en la API de QuickBooks: {e}')\n        return None\n\n\ndef _save_raw_json_to_postgres(records: list, table_name: str):\n    \"\"\"\n    Guarda una lista de dicts completos como JSON en Postgres (columna \u00fanica 'data').\n    \"\"\"\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    # Cada fila es un dict convertido a string JSON\n    df = pd.DataFrame([{'data': json.dumps(record)} for record in records])\n\n    df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n    print(f\"\u2705 Datos guardados en la tabla {table_name} (columna \u00fanica 'data')\")\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Pipeline de extracci\u00f3n y carga:\n    1. Extrae datos de QuickBooks\n    2. Guarda el JSON crudo en Postgres\n    \"\"\"\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    minor_version = 75\n    query = 'select * from Invoice'\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n\n    if data and 'QueryResponse' in data and 'Invoice' in data['QueryResponse']:\n        invoices = data['QueryResponse']['Invoice']\n\n        # Guardar crudo como JSON en columna \u00fanica\n        _save_raw_json_to_postgres(invoices, 'raw_invoices')\n\n        return {\"records_extracted\": len(invoices)}\n    else:\n        return {\"records_extracted\": 0}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Verifica que el pipeline extraiga al menos un registro\n    \"\"\"\n    assert \"records_extracted\" in output, \"El bloque no devolvi\u00f3 la m\u00e9trica esperada\"\n    print(f\"{output['records_extracted']} registros extra\u00eddos y guardados en Postgres\")\n", "file_path": "data_loaders/test2.py", "language": "python", "type": "data_loader", "uuid": "test2"}, "data_loaders/test3.py:data_loader:python:test3": {"content": "import requests\nimport pandas as pd\nimport json\nimport datetime\nimport time\nimport random\nfrom sqlalchemy import create_engine\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n    auth = (client_id, client_secret)\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    resp = requests.post(url, headers=headers, data=data, auth=auth, timeout=30)\n    resp.raise_for_status()\n    return resp.json()['access_token']\n\n\n\"\"\" def _fetch_qb_data(realm_id, access_token, query, base_url, minor_version):\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n    params = {\"query\": query, \"minorversion\": minor_version}\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    response = requests.get(url, headers=headers, params=params, timeout=60)\n    response.raise_for_status()\n    return response.json()\n\"\"\"\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version, page_size=1000, max_retries=5):\n    \"\"\"\n    Llama a la API de QuickBooks con paginaci\u00f3n y backoff exponencial.\n    \n    - Usa STARTPOSITION y MAXRESULTS para recorrer todas las p\u00e1ginas.\n    - Reintenta en caso de error con backoff exponencial.\n    - Devuelve la lista completa de registros.\n    \"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    all_records = []\n    start_position = 1\n    has_more = True\n    page = 1\n\n    while has_more:\n        paged_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        params = {\n            \"query\": paged_query,\n            \"minorversion\": minor_version\n        }\n        url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n        for intento in range(max_retries):\n            try:\n                response = requests.get(url, headers=headers, params=params, timeout=60)\n                response.raise_for_status()\n                data = response.json()\n\n                # Extraer registros de la respuesta\n                records = data.get('QueryResponse', {}).get('Invoice', [])\n                all_records.extend(records)\n\n                print(f\"\u2705 P\u00e1gina {page}: {len(records)} registros obtenidos (posici\u00f3n inicial {start_position})\")\n\n                # Si recibimos menos registros que el page_size \u2192 no hay m\u00e1s p\u00e1ginas\n                has_more = len(records) == page_size\n                break\n\n            except requests.exceptions.RequestException as e:\n                espera = (2 ** intento) + random.uniform(0, 1)\n                print(f\"\u26a0\ufe0f Error en la p\u00e1gina {page} ({e}). Reintentando en {espera:.2f}s...\")\n                time.sleep(espera)\n\n                if intento == max_retries - 1:\n                    print(\"\u274c Fall\u00f3 despu\u00e9s de m\u00faltiples intentos, abortando este tramo.\")\n                    has_more = False\n                    break\n\n        # Avanzar a la siguiente p\u00e1gina\n        start_position += page_size\n        page += 1\n\n    print(f\"\ud83d\udcca Total de registros extra\u00eddos: {len(all_records)}\")\n    return all_records\n\n\ndef _save_raw_json_to_postgres(records: list, table_name: str):\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    df = pd.DataFrame([{'data': json.dumps(record)} for record in records])\n    df.to_sql(table_name, con=engine, if_exists='append', index=False)\n\n\n\"\"\"\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \n    #Pipeline con segmentaci\u00f3n:\n    # Usa fecha_inicio y fecha_fin (ISO: YYYY-MM-DD).\n    # Divide en chunks diarios (puedes cambiar a semanales).\n    # Guarda los datos crudos en Postgres.\n\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    minor_version = 75\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    # Parametros recibidos\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-07')\n\n    start_date = datetime.date.fromisoformat(fecha_inicio)\n    end_date = datetime.date.fromisoformat(fecha_fin)\n\n    results_summary = []\n\n    current_date = start_date\n    while current_date <= end_date:\n        chunk_start = current_date\n        chunk_end = min(chunk_start + datetime.timedelta(days=1), end_date)\n\n        query = f\"select * from Invoice where TxnDate >= '{chunk_start}' and TxnDate <= '{chunk_end}'\"\n\n        print(f\"Procesando rango: {chunk_start} \u2192 {chunk_end}\")\n        start_time = time.time()\n\n        try:\n            data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n            records = data.get('QueryResponse', {}).get('Invoice', [])\n\n            if records:\n                _save_raw_json_to_postgres(records, 'raw_invoices')\n\n            elapsed = time.time() - start_time\n            results_summary.append({\n                \"fecha_inicio\": str(chunk_start),\n                \"fecha_fin\": str(chunk_end),\n                \"filas\": len(records),\n                \"duracion_seg\": round(elapsed, 2),\n            })\n\n            print(f\"\u2705 {len(records)} filas insertadas en {elapsed:.2f} seg.\")\n\n        except Exception as e:\n            print(f\"\u274c Error en el rango {chunk_start} \u2192 {chunk_end}: {e}\")\n\n        current_date = chunk_end + datetime.timedelta(days=1)\n\n    return {\"chunks\": results_summary}\n\"\"\"\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Pipeline con segmentaci\u00f3n y paginaci\u00f3n.\n    - Usa fecha_inicio y fecha_fin (ISO).\n    - Divide en chunks diarios.\n    - Llama a _fetch_qb_data que devuelve una lista de registros.\n    - Guarda en Postgres en formato raw JSON.\n    \"\"\"\n    import datetime, time\n\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    minor_version = 75\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-07')\n\n    start_date = datetime.date.fromisoformat(fecha_inicio)\n    end_date = datetime.date.fromisoformat(fecha_fin)\n\n    results_summary = []\n\n    current_date = start_date\n    while current_date <= end_date:\n        chunk_start = current_date\n        chunk_end = min(chunk_start + datetime.timedelta(days=1), end_date)\n\n        query = f\"select * from Invoice where TxnDate >= '{chunk_start}' and TxnDate <= '{chunk_end}'\"\n\n        print(f\"\\nProcesando rango: {chunk_start} \u2192 {chunk_end}\")\n        start_time = time.time()\n\n        try:\n            # Ahora _fetch_qb_data devuelve directamente una lista de registros\n            records = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n\n            if records:\n                _save_raw_json_to_postgres(records, 'raw_invoices')\n\n            elapsed = time.time() - start_time\n            results_summary.append({\n                \"fecha_inicio\": str(chunk_start),\n                \"fecha_fin\": str(chunk_end),\n                \"filas\": len(records),\n                \"duracion_seg\": round(elapsed, 2),\n            })\n\n            print(f\"\u2705 {len(records)} filas insertadas en {elapsed:.2f} seg.\")\n\n        except Exception as e:\n            print(f\"\u274c Error en el rango {chunk_start} \u2192 {chunk_end}: {e}\")\n\n        current_date = chunk_end + datetime.timedelta(days=1)\n\n    return {\"chunks\": results_summary}\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert \"chunks\" in output, \"El bloque no devolvi\u00f3 los resultados esperados\"\n    print(\"Resumen de chunks procesados:\")\n    for chunk in output[\"chunks\"]:\n        print(chunk)\n", "file_path": "data_loaders/test3.py", "language": "python", "type": "data_loader", "uuid": "test3"}, "data_loaders/test4.py:data_loader:python:test4": {"content": "import requests\nimport json\nimport time\nimport random\nfrom sqlalchemy import create_engine, text\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n    auth = (client_id, client_secret)\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    resp = requests.post(url, headers=headers, data=data, auth=auth, timeout=30)\n    resp.raise_for_status()\n    return resp.json()['access_token']\n\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version,\n                   page_size=1000, max_retries=5):\n    \"\"\"\n    Llama a la API de QuickBooks con paginaci\u00f3n y backoff exponencial.\n    \"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    all_records = []\n    start_position = 1\n    page = 1\n    has_more = True\n\n    while has_more:\n        paged_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        params = {\"query\": paged_query, \"minorversion\": minor_version}\n        url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n        for intento in range(max_retries):\n            try:\n                response = requests.get(url, headers=headers, params=params, timeout=60)\n                response.raise_for_status()\n                data = response.json()\n\n                records = data.get('QueryResponse', {}).get('Invoice', [])\n                all_records.extend(records)\n\n                print(f\"\u2705 P\u00e1gina {page}: {len(records)} registros obtenidos (posici\u00f3n {start_position})\")\n                has_more = len(records) == page_size\n                break\n\n            except requests.exceptions.RequestException as e:\n                espera = (2 ** intento) + random.uniform(0, 1)\n                print(f\"\u26a0\ufe0f Error en la p\u00e1gina {page}: {e}. Reintentando en {espera:.2f}s...\")\n                time.sleep(espera)\n\n                if intento == max_retries - 1:\n                    print(\"\u274c Fall\u00f3 despu\u00e9s de m\u00faltiples intentos, abortando este tramo.\")\n                    has_more = False\n                    break\n\n        start_position += page_size\n        page += 1\n\n    return all_records\n\n\ndef _save_raw_to_postgres(records, table_name, chunk_start, chunk_end, query, page_number, page_size):\n    \"\"\"\n    Guarda registros en tabla raw con upsert e incluye metadatos.\n    \"\"\"\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    # Crear tabla si no existe\n    with engine.begin() as conn:\n        conn.execute(text(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id TEXT PRIMARY KEY,\n            payload JSONB,\n            ingested_at_utc TIMESTAMP,\n            extract_window_start_utc TIMESTAMP,\n            extract_window_end_utc TIMESTAMP,\n            page_number INT,\n            page_size INT,\n            request_payload TEXT\n        )\n        \"\"\"))\n\n    # Insertar con upsert\n    with engine.begin() as conn:\n        for rec in records:\n            conn.execute(text(f\"\"\"\n            INSERT INTO {table_name} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                :id, :payload, :ingested_at_utc,\n                :extract_window_start_utc, :extract_window_end_utc,\n                :page_number, :page_size, :request_payload\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload\n            \"\"\"), {\n                \"id\": rec.get(\"Id\"),\n                \"payload\": json.dumps(rec),\n                \"ingested_at_utc\": datetime.utcnow(),\n                \"extract_window_start_utc\": chunk_start,\n                \"extract_window_end_utc\": chunk_end,\n                \"page_number\": page_number,\n                \"page_size\": page_size,\n                \"request_payload\": query\n            })\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Pipeline completo para Invoices:\n    - Refresca token\n    - Extrae con paginaci\u00f3n y backoff\n    - Guarda capa raw con metadatos y upsert\n    \"\"\"\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-07')\n\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    minor_version = 75\n\n    start = datetime.fromisoformat(fecha_inicio)\n    end = datetime.fromisoformat(fecha_fin)\n\n    resumen = []\n\n    chunk_start = start\n    while chunk_start <= end:\n        chunk_end = min(chunk_start + timedelta(days=1), end)\n        query = f\"select * from Invoice where TxnDate >= '{chunk_start.date()}' and TxnDate <= '{chunk_end.date()}'\"\n\n        start_time = time.time()\n        records = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        duration = round(time.time() - start_time, 2)\n\n        _save_raw_to_postgres(records, \"qb_invoices\",\n                              chunk_start, chunk_end, query,\n                              page_number=1, page_size=len(records))\n\n        resumen.append({\n            \"fecha_inicio\": chunk_start.isoformat(),\n            \"fecha_fin\": chunk_end.isoformat(),\n            \"filas\": len(records),\n            \"duracion_seg\": duration\n        })\n\n        chunk_start = chunk_end + timedelta(days=1)\n\n    print(\"\ud83d\udcca Resumen:\")\n    for r in resumen:\n        print(r)\n\n    return resumen\n", "file_path": "data_loaders/test4.py", "language": "python", "type": "data_loader", "uuid": "test4"}, "data_loaders/test_dos_pipelines_faltantes.py:data_loader:python:test dos pipelines faltantes": {"content": "import requests\nimport json\nimport time\nimport random\nfrom sqlalchemy import create_engine, text\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n    auth = (client_id, client_secret)\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    resp = requests.post(url, headers=headers, data=data, auth=auth, timeout=30)\n    resp.raise_for_status()\n    return resp.json()['access_token']\n\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version,\n                   entity, page_size=1000, max_retries=5):\n    \"\"\"\n    Llama a la API de QuickBooks con paginaci\u00f3n y backoff exponencial.\n    \"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    all_records = []\n    start_position = 1\n    page = 1\n    has_more = True\n\n    while has_more:\n        paged_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        params = {\"query\": paged_query, \"minorversion\": minor_version}\n        url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n        for intento in range(max_retries):\n            try:\n                response = requests.get(url, headers=headers, params=params, timeout=60)\n                response.raise_for_status()\n                data = response.json()\n\n                # Aqu\u00ed lo hacemos din\u00e1mico\n                records = data.get('QueryResponse', {}).get(entity, [])\n                all_records.extend(records)\n\n                print(f\"\u2705 P\u00e1gina {page}: {len(records)} registros obtenidos (posici\u00f3n {start_position})\")\n                has_more = len(records) == page_size\n                break\n\n            except requests.exceptions.RequestException as e:\n                espera = (2 ** intento) + random.uniform(0, 1)\n                print(f\"\u26a0\ufe0f Error en la p\u00e1gina {page}: {e}. Reintentando en {espera:.2f}s...\")\n                time.sleep(espera)\n\n                if intento == max_retries - 1:\n                    print(\"\u274c Fall\u00f3 despu\u00e9s de m\u00faltiples intentos, abortando este tramo.\")\n                    has_more = False\n                    break\n\n        start_position += page_size\n        page += 1\n\n    return all_records  \n\n\ndef _save_raw_to_postgres(records, table_name, chunk_start, chunk_end, query, page_number, page_size):\n    \"\"\"\n    Guarda registros en tabla raw con upsert e incluye metadatos.\n    \"\"\"\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    # Crear tabla si no existe\n    with engine.begin() as conn:\n        conn.execute(text(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id TEXT PRIMARY KEY,\n            payload JSONB,\n            ingested_at_utc TIMESTAMP,\n            extract_window_start_utc TIMESTAMP,\n            extract_window_end_utc TIMESTAMP,\n            page_number INT,\n            page_size INT,\n            request_payload TEXT\n        )\n        \"\"\"))\n\n    # Insertar con upsert\n    with engine.begin() as conn:\n        for rec in records:\n            conn.execute(text(f\"\"\"\n            INSERT INTO {table_name} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                :id, :payload, :ingested_at_utc,\n                :extract_window_start_utc, :extract_window_end_utc,\n                :page_number, :page_size, :request_payload\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload\n            \"\"\"), {\n                \"id\": rec.get(\"Id\"),\n                \"payload\": json.dumps(rec),\n                \"ingested_at_utc\": datetime.utcnow(),\n                \"extract_window_start_utc\": chunk_start,\n                \"extract_window_end_utc\": chunk_end,\n                \"page_number\": page_number,\n                \"page_size\": page_size,\n                \"request_payload\": query\n            })\n\n\n@data_loader\ndef load_data_from_customers(*args, **kwargs):\n    \"\"\"\n    Pipeline completo para Customers:\n    - Refresca token\n    - Extrae con paginaci\u00f3n y backoff\n    - Guarda capa raw con metadatos y upsert\n    \"\"\"\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    minor_version = 75\n\n    query = \"select * from Customer\"\n\n    start_time = time.time()\n    records = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version, entity=\"Customer\")\n    duration = round(time.time() - start_time, 2)\n\n    _save_raw_to_postgres(records, \"qb_customers\",\n                          chunk_start=datetime.utcnow(),\n                          chunk_end=datetime.utcnow(),\n                          query=query,\n                          page_number=1,\n                          page_size=len(records))\n\n    resumen = {\n        \"entidad\": \"Customer\",\n        \"filas\": len(records),\n        \"duracion_seg\": duration\n    }\n\n    print(\"\ud83d\udcca Resumen Customers:\")\n    print(resumen)\n\n    return resumen\n", "file_path": "data_loaders/test_dos_pipelines_faltantes.py", "language": "python", "type": "data_loader", "uuid": "test_dos_pipelines_faltantes"}, "data_loaders/test_pipeline_generico.py:data_loader:python:test pipeline generico": {"content": "import requests\nimport json\nimport time\nimport random\nfrom sqlalchemy import create_engine, text\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n    auth = (client_id, client_secret)\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    resp = requests.post(url, headers=headers, data=data, auth=auth, timeout=30)\n    resp.raise_for_status()\n    return resp.json()['access_token']\n\n\ndef _fetch_qb_data(realm_id, access_token, entidad, query, base_url, minor_version,\n                   page_size=1000, max_retries=5):\n    \"\"\"\n    Llama a la API de QuickBooks con paginaci\u00f3n y backoff exponencial.\n    \"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    all_records = []\n    start_position = 1\n    page = 1\n    has_more = True\n\n    while has_more:\n        paged_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        params = {\"query\": paged_query, \"minorversion\": minor_version}\n        url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n        for intento in range(max_retries):\n            try:\n                response = requests.get(url, headers=headers, params=params, timeout=60)\n                response.raise_for_status()\n                data = response.json()\n\n                # Aqu\u00ed cambia seg\u00fan la entidad\n                records = data.get('QueryResponse', {}).get(entidad, [])\n                all_records.extend(records)\n\n                print(f\"\u2705 P\u00e1gina {page}: {len(records)} registros obtenidos (posici\u00f3n {start_position})\")\n                has_more = len(records) == page_size\n                break\n\n            except requests.exceptions.RequestException as e:\n                espera = (2 ** intento) + random.uniform(0, 1)\n                print(f\"\u26a0\ufe0f Error en la p\u00e1gina {page}: {e}. Reintentando en {espera:.2f}s...\")\n                time.sleep(espera)\n\n                if intento == max_retries - 1:\n                    print(\"\u274c Fall\u00f3 despu\u00e9s de m\u00faltiples intentos, abortando este tramo.\")\n                    has_more = False\n                    break\n\n        start_position += page_size\n        page += 1\n\n    return all_records\n\n\ndef _save_raw_to_postgres(records, table_name, chunk_start, chunk_end, query, page_number, page_size):\n    \"\"\"\n    Guarda registros en tabla raw con upsert e incluye metadatos.\n    \"\"\"\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    # Crear tabla si no existe\n    with engine.begin() as conn:\n        conn.execute(text(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id TEXT PRIMARY KEY,\n            payload JSONB,\n            ingested_at_utc TIMESTAMP,\n            extract_window_start_utc TIMESTAMP,\n            extract_window_end_utc TIMESTAMP,\n            page_number INT,\n            page_size INT,\n            request_payload TEXT\n        )\n        \"\"\"))\n\n    # Insertar con upsert\n    with engine.begin() as conn:\n        for rec in records:\n            conn.execute(text(f\"\"\"\n            INSERT INTO {table_name} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                :id, :payload, :ingested_at_utc,\n                :extract_window_start_utc, :extract_window_end_utc,\n                :page_number, :page_size, :request_payload\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload\n            \"\"\"), {\n                \"id\": rec.get(\"Id\"),\n                \"payload\": json.dumps(rec),\n                \"ingested_at_utc\": datetime.utcnow(),\n                \"extract_window_start_utc\": chunk_start,\n                \"extract_window_end_utc\": chunk_end,\n                \"page_number\": page_number,\n                \"page_size\": page_size,\n                \"request_payload\": query\n            })\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Pipeline gen\u00e9rico para QuickBooks:\n    - Refresca token\n    - Extrae con paginaci\u00f3n y backoff\n    - Guarda capa raw con metadatos y upsert\n    \"\"\"\n    entidad = kwargs.get('entidad', 'Item')   # \ud83d\udc48 Cambia aqu\u00ed: Invoice, Customer, Item\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-07')\n\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    minor_version = 75\n\n    start = datetime.fromisoformat(fecha_inicio)\n    end = datetime.fromisoformat(fecha_fin)\n\n    resumen = []\n\n    chunk_start = start\n    while chunk_start <= end:\n        chunk_end = min(chunk_start + timedelta(days=1), end)\n        # \ud83d\udc47 Cambia la condici\u00f3n: Invoices tienen TxnDate, Items usan MetaData.LastUpdatedTime\n        if entidad == \"Invoice\":\n            print(\"Haciendo como Invoice\")\n            query = f\"select * from {entidad} where TxnDate >= '{chunk_start.date()}' and TxnDate <= '{chunk_end.date()}'\"\n        else:\n            query = f\"select * from {entidad}\"\n\n        start_time = time.time()\n        records = _fetch_qb_data(realm_id, access_token, entidad, query, base_url, minor_version)\n        duration = round(time.time() - start_time, 2)\n\n        _save_raw_to_postgres(records, f\"qb_{entidad.lower()}\",\n                              chunk_start, chunk_end, query,\n                              page_number=1, page_size=len(records))\n\n        resumen.append({\n            \"entidad\": entidad,\n            \"fecha_inicio\": chunk_start.isoformat(),\n            \"fecha_fin\": chunk_end.isoformat(),\n            \"filas\": len(records),\n            \"duracion_seg\": duration\n        })\n\n        chunk_start = chunk_end + timedelta(days=1)\n\n    print(\"\ud83d\udcca Resumen:\")\n    for r in resumen:\n        print(r)\n\n    return resumen\n", "file_path": "data_loaders/test_pipeline_generico.py", "language": "python", "type": "data_loader", "uuid": "test_pipeline_generico"}, "pipelines/tests1/metadata.yaml:pipeline:yaml:tests1/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Test2\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test2\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test3\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test3\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test4\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test4\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test_dos_pipelines_faltantes\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test_dos_pipelines_faltantes\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test_pipeline_generico\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test_pipeline_generico\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-07 02:33:04.916740+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Tests1\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: tests1\nvariables_dir: /home/src/default\nwidgets: []\n", "file_path": "pipelines/tests1/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "tests1/metadata"}, "pipelines/tests1/__init__.py:pipeline:python:tests1/  init  ": {"content": "", "file_path": "pipelines/tests1/__init__.py", "language": "python", "type": "pipeline", "uuid": "tests1/__init__"}, "pipelines/tranquil_phoenix/metadata.yaml:pipeline:yaml:tranquil phoenix/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - data_esporter_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Data Loader Invoices\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_loader_invoices\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_esporter_invoices\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_loader_invoices\n  uuid: data_esporter_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 03:53:38.515802+00:00'\ndata_integration: null\ndescription: Test2\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: tranquil phoenix\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: tranquil_phoenix\nvariables_dir: /home/src/default\nwidgets: []\n", "file_path": "pipelines/tranquil_phoenix/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "tranquil_phoenix/metadata"}, "pipelines/tranquil_phoenix/__init__.py:pipeline:python:tranquil phoenix/  init  ": {"content": "", "file_path": "pipelines/tranquil_phoenix/__init__.py", "language": "python", "type": "pipeline", "uuid": "tranquil_phoenix/__init__"}, "/home/src/default/data_exporters/qb_pipeline_export.py:data_exporter:python:home/src/default/data exporters/qb pipeline export": {"content": "import requests\nimport json\nimport time\nimport random\nfrom sqlalchemy import create_engine, text\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\ndef _save_raw_to_postgres(records, table_name, chunk_start, chunk_end, query, page_number, page_size, request_payloads):\n    # M\u00e9todo para guardar registros en tabla raw con upsert e incluir metadatos\n\n    user = get_secret_value('pg_user')\n    password = get_secret_value('pg_password')\n    host = get_secret_value('pg_host')\n    port = get_secret_value('pg_port')\n    db = get_secret_value('pg_db')\n\n    url_conn = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n    engine = create_engine(url_conn)\n\n    # Asegurar esquema raw\n    with engine.begin() as conn:\n        conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS raw\"))\n\n    # Crear tabla si no existe\n    with engine.begin() as conn:\n        conn.execute(text(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id TEXT PRIMARY KEY,\n            payload JSONB,\n            ingested_at_utc TIMESTAMP,\n            extract_window_start_utc TIMESTAMP,\n            extract_window_end_utc TIMESTAMP,\n            page_number INT,\n            page_size INT,\n            request_payload TEXT\n        )\n        \"\"\"))\n\n    # Insertar con upsert\n    with engine.begin() as conn:\n        for rec in records:\n            conn.execute(text(f\"\"\"\n            INSERT INTO {table_name} (\n                id, payload, ingested_at_utc,\n                extract_window_start_utc, extract_window_end_utc,\n                page_number, page_size, request_payload\n            )\n            VALUES (\n                :id, :payload, :ingested_at_utc,\n                :extract_window_start_utc, :extract_window_end_utc,\n                :page_number, :page_size, :request_payload\n            )\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number,\n                page_size = EXCLUDED.page_size,\n                request_payload = EXCLUDED.request_payload\n            \"\"\"), {\n                \"id\": rec.get(\"Id\"),\n                \"payload\": json.dumps(rec),\n                \"ingested_at_utc\": datetime.utcnow(),\n                \"extract_window_start_utc\": chunk_start,\n                \"extract_window_end_utc\": chunk_end,\n                \"page_number\": page_number,\n                \"page_size\": page_size,\n                \"request_payload\": json.dumps(request_payloads)\n            })\n\n\n@data_exporter\ndef export_data_to_postgres(data, *args, **kwargs):\n    # Data exporter:\n    # Recibe la salida del data_loader\n    # Inserta en Postgres cada chunk con sus metadatos\n\n    resultados = data[\"resultados\"]\n\n    for i, chunk in enumerate(resultados, 1):\n        records = chunk[\"records\"]\n        if not records:\n            continue\n\n        _save_raw_to_postgres(\n            records=records,\n            table_name=f\"qb_{chunk['entidad'].lower()}s\",\n            chunk_start=chunk[\"chunk_start\"],\n            chunk_end=chunk[\"chunk_end\"],\n            query=chunk[\"query\"],\n            page_number=1,\n            page_size=len(records),\n            request_payloads=chunk[\"request_payloads\"] \n        )\n        print(f\"\u2705 Chunk {i} ({chunk['entidad']}) guardado en Postgres con {len(records)} filas\")\n", "file_path": "/home/src/default/data_exporters/qb_pipeline_export.py", "language": "python", "type": "data_exporter", "uuid": "qb_pipeline_export"}, "/home/src/default/data_loaders/pipeline_generico.py:data_loader:python:home/src/default/data loaders/pipeline generico": {"content": "# Importaci\u00f3n de librer\u00edas a utilizar\nimport requests\nimport json\nimport time\nimport random\nfrom sqlalchemy import create_engine, text\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\n\n# ifs obtenidos de la clase para funcionalidad del m\u00e9todo data_loader al final\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Varios de los m\u00e9todos se basan en los vistos en clase\n# Asimismo, algunos cambios y mejoras para cumplir todos los literales del proyecto se realizaron con una investigaci\u00f3n en otras p\u00e1ginas\n# y con ayuda de ChatGPT como una herramienta gu\u00eda\ndef refresh_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Accept\": \"application/json\", \n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n    auth = (client_id, client_secret)\n    data = {\n        \"grant_type\": \"refresh_token\", \n        \"refresh_token\": refresh_token\n    }\n\n    resp = requests.post(url, headers=headers, data=data, auth=auth, timeout=30)\n    #print(resp.status_code, resp.text)\n    resp.raise_for_status()\n    tokens = resp.json()\n\n    new_refresh = tokens[\"refresh_token\"]\n    new_access = tokens[\"access_token\"]\n\n    print(\"Nuevo refresh_token:\", new_refresh)\n\n    return new_access\n\ndef _fetch_qb_data(realm_id, access_token, entidad, query, base_url, minor_version,\n                   page_size=1000, max_retries=5):\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    all_records = []\n    start_position = 1\n    page = 1\n    has_more = True\n    request_payloads = []\n\n    while has_more:\n        paged_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        params = {\"query\": paged_query, \"minorversion\": minor_version}\n        url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n        # Guardar request payload (SIN Authorization)\n        request_payloads.append({\n            \"url\": url,\n            \"headers\": {k: v for k, v in headers.items() if k.lower() != \"authorization\"},\n            \"params\": params\n        })\n\n        for intento in range(max_retries):\n            try:\n                response = requests.get(url, headers=headers, params=params, timeout=60)\n                response.raise_for_status()\n                data = response.json()\n\n                records = data.get('QueryResponse', {}).get(entidad, [])\n                all_records.extend(records)\n\n                print(f\"\u2705 P\u00e1gina {page}: {len(records)} registros obtenidos (posici\u00f3n {start_position})\")\n                has_more = len(records) == page_size\n                break\n\n            except requests.exceptions.RequestException as e:\n                espera = (2 ** intento) + random.uniform(0, 1)\n                print(f\"\u26a0\ufe0f Error en la p\u00e1gina {page}: {e}. Reintentando en {espera:.2f}s...\")\n                time.sleep(espera)\n\n                if intento == max_retries - 1:\n                    print(\"\u274c Fall\u00f3 despu\u00e9s de m\u00faltiples intentos, abortando este tramo.\")\n                    has_more = False\n                    break\n\n        start_position += page_size\n        page += 1\n\n    return all_records, request_payloads\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    # Data loader:\n    # - Extrae registros de QuickBooks en tramos (chunking)\n    # Devuelve los registros y el resumen (sin persistir en DB - Eso se hace en el data exporter)\n\n    entidad = kwargs.get('entidad', 'Invoice') # Invoice, Customer, Item\n    fecha_inicio = kwargs.get('fecha_inicio', '2025-08-01')\n    fecha_fin = kwargs.get('fecha_fin', '2025-08-07')\n\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = refresh_access_token()\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    minor_version = 75\n\n    start = datetime.fromisoformat(fecha_inicio)\n    end = datetime.fromisoformat(fecha_fin)\n\n    all_results = []   # Registros\n    resumen = []       # M\u00e9tricas - Para el apartado adicional de mostrar un resumen\n\n    chunk_start = start\n    while chunk_start <= end:\n        chunk_end = min(chunk_start + timedelta(days=1), end)\n        if entidad == \"Invoice\":\n            print(\"Haciendo como Invoice\")\n            query = f\"select * from {entidad} where TxnDate >= '{chunk_start.date()}' and TxnDate <= '{chunk_end.date()}'\"\n        else:\n            query = f\"select * from {entidad}\"\n\n        start_time = time.time()\n        records, request_payloads = _fetch_qb_data(\n            realm_id, access_token, entidad, query, base_url, minor_version\n        )\n        duration = round(time.time() - start_time, 2)\n\n        # Acumular los resultados para las tablas\n        all_results.append({\n            \"entidad\": entidad,\n            \"chunk_start\": chunk_start,\n            \"chunk_end\": chunk_end,\n            \"records\": records,\n            \"query\": query,\n            \"request_payloads\": request_payloads\n        })\n\n        resumen.append({\n            \"entidad\": entidad,\n            \"fecha_inicio\": chunk_start.isoformat(),\n            \"fecha_fin\": chunk_end.isoformat(),\n            \"filas\": len(records),\n            \"duracion_seg\": duration\n        })\n\n        chunk_start = chunk_end + timedelta(days=1)\n\n    print(\"\ud83d\udcca Resumen:\")\n    for r in resumen:\n        print(r)\n\n    return {\n        \"resultados\": all_results,\n        \"resumen\": resumen\n    } ", "file_path": "/home/src/default/data_loaders/pipeline_generico.py", "language": "python", "type": "data_loader", "uuid": "pipeline_generico"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}